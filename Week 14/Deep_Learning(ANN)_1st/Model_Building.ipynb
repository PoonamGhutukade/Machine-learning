{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Tensorflow Model for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessing file \n",
    "file = open('CSV_files/Preprocessing_file.pickle','rb')\n",
    "x_train,y_train,x_cv,y_cv = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\"\"\"\n",
    "we are fitting and transforming the training data using the StandardScaler function.\n",
    "We standardize our scaling so that we use the same fitted method to transform/scale test data. \n",
    "\"\"\"\n",
    "sc = StandardScaler() \n",
    "x_train = sc.fit_transform(x_train) \n",
    "x_cv = sc.transform(x_cv) \n",
    "# Data scaled properly. And done with preprocessing \n",
    "# If we fit_tranform on train data then no need to fit it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6400, 11), (6400,), (1600, 11), (1600,), numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_cv.shape, y_cv.shape,type(x_train), type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.57735027, -0.57494463, -2.42587752, ..., -1.54127461,\n",
       "        -1.04605967, -1.57549058],\n",
       "       [-0.57735027,  1.73929791, -2.27938387, ...,  0.64881365,\n",
       "        -1.04605967,  0.88573347],\n",
       "       [-0.57735027, -0.57494463,  0.8493018 , ...,  0.64881365,\n",
       "        -1.04605967,  1.5629973 ],\n",
       "       ...,\n",
       "       [-0.57735027, -0.57494463,  0.76559115, ...,  0.64881365,\n",
       "         0.95596841,  0.60626778],\n",
       "       [-0.57735027, -0.57494463, -0.18661753, ..., -1.54127461,\n",
       "         0.95596841, -1.69645522],\n",
       "       [ 1.73205081, -0.57494463,  0.37842938, ...,  0.64881365,\n",
       "        -1.04605967, -0.60801275]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 11), (1600,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cv.shape, y_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.57735027, -0.57494463,  0.95394012, ...,  0.64881365,\n",
       "        -1.04605967, -1.09701775],\n",
       "       [-0.57735027, -0.57494463, -0.68888146, ...,  0.64881365,\n",
       "        -1.04605967,  0.24720832],\n",
       "       [-0.57735027, -0.57494463, -1.73526462, ...,  0.64881365,\n",
       "        -1.04605967,  1.11938958],\n",
       "       ...,\n",
       "       [-0.57735027,  1.73929791,  1.18414442, ...,  0.64881365,\n",
       "         0.95596841, -1.7164863 ],\n",
       "       [-0.57735027, -0.57494463, -1.42134967, ...,  0.64881365,\n",
       "         0.95596841, -1.66739247],\n",
       "       [-0.57735027,  1.73929791, -0.50053248, ..., -1.54127461,\n",
       "         0.95596841,  0.92993369]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test: (2000, 11) y_test (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Import test datset\n",
    "# Loading testing file from pickle file\n",
    "test_file = open(\"CSV_files/Testing_file.pickle\",\"rb\")\n",
    "x_test = pickle.load(test_file)\n",
    "y_test = pickle.load(test_file) \n",
    "print(\"x_test:\",x_test.shape,\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Model save and restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unknown> <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Epoch 0 completed out of 10 loss: 16367.829956054688\n",
      "Epoch 1 completed out of 10 loss: 15008.055908203125\n",
      "Epoch 2 completed out of 10 loss: 14128.538696289062\n",
      "Epoch 3 completed out of 10 loss: 13420.9560546875\n",
      "Epoch 4 completed out of 10 loss: 12861.303588867188\n",
      "Epoch 5 completed out of 10 loss: 12404.52685546875\n",
      "Epoch 6 completed out of 10 loss: 12021.93408203125\n",
      "Epoch 7 completed out of 10 loss: 11690.542846679688\n",
      "Epoch 8 completed out of 10 loss: 11388.918579101562\n",
      "Epoch 9 completed out of 10 loss: 11105.563354492188\n",
      "Accuracy: 50.1875\n",
      "Model saved in file: ANN_model_files/model.ckpt\n",
      "\n",
      "\n",
      "Starting 2nd session...\n",
      "INFO:tensorflow:Restoring parameters from ANN_model_files/model.ckpt\n",
      "Model restored from file: ANN_model_files/model.ckpt\n",
      "Epoch: 0001 cost= 1084.553393555\n",
      "Epoch: 0002 cost= 1060.544580078\n",
      "Epoch: 0003 cost= 1037.455688477\n",
      "Epoch: 0004 cost= 1015.053961182\n",
      "Epoch: 0005 cost= 993.342773438\n",
      "Epoch: 0006 cost= 972.595397949\n",
      "Epoch: 0007 cost= 952.248388672\n",
      "Epoch: 0008 cost= 932.664776611\n",
      "Epoch: 0009 cost= 913.174182129\n",
      "Epoch: 0010 cost= 893.759680176\n",
      "Second Optimization Finished!\n",
      "Accuracy for test: 100.0\n"
     ]
    }
   ],
   "source": [
    "# number of nodes in each layer\n",
    "y_train = y_train.reshape(6400,1)\n",
    "y_cv = y_cv.reshape(1600, 1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "# Neurons in hidden layer\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "n_classes = 2\n",
    "display_step = 1\n",
    "batch_size = 100\n",
    "\n",
    "model_path = \"ANN_model_files/model.ckpt\"\n",
    "\n",
    "\"\"\"A placeholder is simply a variable that we will assign data to at a later date. \n",
    "It allows us to create our operations and build our computation graph, without needing the data. \n",
    "In Tens\"\"\"\n",
    "x = tf.placeholder('float', [None, x_train.shape[1]])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def neural_network_model(data):\n",
    "    # Weight = x_columns * neurons in 1st layer\n",
    "    # Bias = neurons in 1st layer * 1\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([x_train.shape[1], n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    # Weight = neurons in 1st layer * neurons in 2nd layer\n",
    "    # Bias = neurons in 2nd layer * 1 ..so on\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                    'biases':tf.Variable(tf.random_normal([n_classes])),}\n",
    "\n",
    "    \n",
    "    # result = ((data * weights) + Bias)\n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    # activation (relu) function\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output\n",
    "\n",
    "# train ANN model using tensorflow\n",
    "def train_neural_network(x):\n",
    "    # Here we call neural_network_model() function & passing variable x\n",
    "    prediction = neural_network_model(x)\n",
    "#     print(\"prediction.....\",prediction)\n",
    "    \"\"\"\n",
    "    ..The functionality of numpy.mean and tensorflow.reduce_mean are the same. They do the same thing.\n",
    "    ..tf.nn.softmax produces just the result of applying the softmax function to an input tensor.\n",
    "    ..Softmax is often used in neural networks, to map the non-normalized output of a network to a\n",
    "    probability distribution over predicted output classes.\n",
    "    \n",
    "    ..Cost variable.\n",
    "    .This measures how wrong we are, and is the variable we desire to minimize by manipulating our weights\n",
    "    .To optimize our cost, we will use the AdamOptimizer, which is a popular optimizer along with others \n",
    "    like Stochastic Gradient Descent and AdaGrad\n",
    "    \"\"\"\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels = y) )\n",
    "#     print(\"cost.....\",cost)\n",
    "    \"\"\"\n",
    "    Within AdamOptimizer(), you can optionally specify the learning_rate as a parameter. \n",
    "    The default is 0.001, which is fine for most circumstances.\n",
    "    optimizer.minimize(cost) is creating new values & variables in your graph.\n",
    "    \"\"\"\n",
    "    # Add the optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "#     cost = tf.get_variable(cost)\n",
    "#     prediction = tf.get_variable(prediction)\n",
    "\n",
    "    \"\"\"\n",
    "    hm_epochs variable which will determine how many epochs to have (cycles of feed forward and back prop\n",
    "    \"\"\"\n",
    "    hm_epochs = 10\n",
    "    ##########################\n",
    "    saver = tf.train.Saver()\n",
    "#     saver = tf.train.Saver([prediction, cost])\n",
    "    \n",
    "    # launch the graph in a session\n",
    "    with tf.Session() as sess:\n",
    "        #These variables must be initialized before you can train a model.\n",
    "        # it only add ops(AdamOptimizer) that will initialize the variables (i.e. assign their default value) when run.\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(10):\n",
    "                # Actually intialize the variables\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x:x_train, y:y_train})\n",
    "                epoch_loss += c\n",
    "            \n",
    "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "#         c = tf.get_variable(c)\n",
    "#         saver = tf.train.Saver([c])\n",
    "        # Save the model \n",
    "        \"\"\"\n",
    "        # This will save following files in Tensorflow v >= 0.11 (For next version than .11)\n",
    "        # my_test_model.data-00000-of-00001\n",
    "        # my_test_model.index\n",
    "        # my_test_model.meta\n",
    "        # checkpoint\n",
    "        \"\"\"\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_cv, 1))\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float')) *100\n",
    "        print('Accuracy:',accuracy.eval({x:x_cv, y:y_cv}))\n",
    "#         print('Accuracy:',accuracy.eval({x:x_test, y:y_test}))\n",
    "        \n",
    "        ##############################\n",
    "         # Save model weights to disk\n",
    "#         saver.save(sess,  'ANN_model_files/my_test_model')\n",
    "        save_path = saver.save(sess, model_path)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "        \n",
    "        \n",
    "        # Running a new session\n",
    "    print(\"\\n\\nStarting 2nd session...\")\n",
    "    init = tf.initialize_all_variables()\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize variables\n",
    "        sess.run(init)\n",
    "\n",
    "        # Restore model weights from previously saved model\n",
    "        saver.restore(sess, model_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)\n",
    "\n",
    "        # Resume training\n",
    "        for epoch in range(10):\n",
    "            avg_cost = 0.\n",
    "            batch_size = 100\n",
    "    #         total_batch = int(mnist.train.num_examples / batch_size)\n",
    "            # Loop over all batches\n",
    "    #         for i in range(total_batch):\n",
    "            for i in range(10):\n",
    "#                 batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: x_train,y: y_train})\n",
    "                # Compute average loss\n",
    "#                 avg_cost += c / total_batch\n",
    "                avg_cost += c / 10\n",
    "            \n",
    "            # Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\",\"{:.9f}\".format(avg_cost))\n",
    "        print(\"Second Optimization Finished!\")\n",
    "\n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_test, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))*100\n",
    "    #     print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "        print('Accuracy for test:',accuracy.eval({x:x_test, y:y_test}))\n",
    "\n",
    "train_neural_network(x)\n",
    "# print(type(correct))\n",
    "# print(x_data.shape, y_data.shape, type(x_cv), type(x_cv))\n",
    "# print(int(mnist.train.num_examples/batch_size)) = 550\n",
    "# print(int(mnist.train.num_examples))\n",
    "# print(type(neural_network_model(x)))\n",
    "# epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "# print(type(epoch_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
